---
layout: default
title: Welcome to MMLoSo 2025
---

<style>
body {
  max-width: 80%;
  margin-left: 10%;
  margin-right: 10%;
}

.wrapper {
  max-width: 900px;
  margin-left: 8%;
  margin-right: auto;
}
</style>

<!-- Top Navigation -->
<nav style="margin-left: 10%; margin-bottom: 2em;">
  <a href="#about" style="margin: 0 15px;">About</a>
  <a href="#cfp" style="margin: 0 15px;">Call for Papers</a>
  <a href="#dates" style="margin: 0 15px;">Important Dates</a>
  <a href="#organizers" style="margin: 0 15px;">Organizers</a>
  <a href="#contact" style="margin: 0 15px;">Contact</a>
</nav>

# MMLoSo 2025  
**Multimodal Models for Low-Resource Contexts and Social Impact**  
ğŸ“ Co-located with IJCNLP-AACL 2025  
ğŸ“… December 2025, Mumbai, India

---

## ğŸ“– <a id="about"></a> About

This workshop brings together researchers at the intersection of multimodal learning, NLP, and AI for social good, with a focus on low-resource and underserved settings. While multimodal dataâ€”text, audio, visual, and sensorâ€”continues to grow, most state-of-the-art models depend on large, high-quality datasets, limiting their applicability in data-scarce environments.

We invite papers on developing robust and inclusive multimodal systems that can operate effectively under data constraints. Topics include learning with multiple modalities, cross-lingual, cross-modal adaptation, and interpretable models for domains like healthcare, ecological monitoring, education, and cultural heritage preservation. Alongside papers and keynotes, a community-driven shared task will evaluate multimodal robustness and generalization in low-resource contexts. 


---

## ğŸ“¢ <a id="cfp"></a> Call for Papers

We focus on bridging the gap between the growing capabilities of multimodal machine learning models and the urgent needs of real-world applications in under-resourced, marginalized, or data-constrained settings. This includes scenarios where data is scarce, modalities are incomplete or imbalanced, and computational or human infrastructure may be limited.

The topics of interest for the workshop include, but are not limited to:

- **Learning with Missing or Incomplete Modalities**  
  Techniques for modality dropout, hallucination, and imputation when input signals are sparse or missing at training or inference time.

- **Few-Shot, Zero-Shot, and Transfer Learning in Multimodal Contexts**  
  Approaches that allow models pre-trained on high-resource datasets to adapt effectively to novel, low-resource domains and languages.

- **Multilingual and Multimodal Representation Learning**  
  Unifying language, vision, audio, and other modalities across multiple languages, especially those underrepresented in current benchmarks.

- **Ethical, Interpretable, and Responsible AI for Multimodal Systems**  
  Auditing and mitigating bias in multimodal systems; developing transparent models that explain decisions across modalities in high-stakes domains.

- **Benchmarking and Evaluation for Real-World Robustness**  
  Proposing new datasets, metrics, and evaluations that reflect deployment challenges in regions with limited resources or infrastructure.

- **Applications in Social Good**, including:
  - Ecological and biodiversity monitoring (e.g., combining satellite, image, and audio data)
  - Public health and epidemiology in underserved regions
  - Language documentation and cultural preservation
  - Crisis response, misinformation detection, and social justice

---

## ğŸ“… <a id="dates"></a> Important Dates  
*(Subject to change based on IJCNLP-AACL 2025)*

- ğŸ“ Submission Deadline: **September 29, 2025**  
- ğŸ“¢ Notification: **November 3, 2025**  
- ğŸ–‹ Camera-ready Deadline: **November 11, 2025**  
- ğŸ“ Workshop: **December 2025**

---

## ğŸ‘¥ <a id="organizers"></a> Organizers

- Ankita Shukla (University of Nevada, Reno)  
- Sandeep Kumar (IIT Delhi)  
- Amrit Singh Bedi (University of Central Florida)  
- Tanmoy Chakraborty (IIT Delhi)

---

## âœ‰ï¸ <a id="contact"></a> Contact

For any queries, email us at:  
ğŸ“§ [ankitas@unr.edu](mailto:ankitas@unr.edu)
